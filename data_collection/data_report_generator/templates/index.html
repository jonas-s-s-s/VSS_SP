{% extends "base.html" %}
{% block content %}

<h1>Benchmark Results</h1>

<table class="pure-table pure-table-bordered">
    <thead>
    <tr>
        <th>Framework</th>
        <th>Test Case Run Count</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>All Frameworks</td>
        <td>{{ total_run_count }}</td>
    </tr>
    {% for measurement in measurement_run_count %}
    <tr>
        <td>{{ measurement.name }}</td>
        <td>{{ measurement.run_count }}</td>
    </tr>
    {% endfor %}
    </tbody>
</table>

<h2>Used Terms</h2>

<p><i>Test case</i> is a single run of the client script which sends HTTP requests to the server, there can be multiple types of test cases varying by the amount of sent requests, the duration of the test, etc.</p>
<p><i>Test mode</i> is the mode in which the server is operating, there are various "modes" prepared for each framework, like SQL mode (the server is querying data from a SQL database), JSON mode, etc.</p>

<h2>Framework Results Summary</h2>

<p>Table Legend:</p>
<ul>
    <li>LAT = Latency</li>
    <li>RPS = Requests Per Second</li>
    <li>P50, P75, ... = Percentiles</li>
    <li>P50, P75, ... = Percentiles</li>
    <li>REQ 2XX = Amount of successful HTTP requests</li>
    <li>REQ TOTAL = Total amount of HTTP requests, including unsuccessful ones</li>
    <li>CPU % = Mean server CPU utilization in %, can go over 100% on multicore systems</li>
    <li>MEM MB = Mean server memory utilization in megabytes</li>
    <li style="margin-top: 0.5em">All latency values are in &#181;s (Microseconds)</li>
</ul>
<p>The following table contains the <b>mean results</b> for each framework over <b>all test cases</b> and <b>test modes</b>. I.e. the mean is computed over all {{ total_run_count }} test case runs.</p>

<div id="benchmark-summary-table"></div>

<p>Visualisations of selected rows from this table can be seen below.</p>

<h3>Mean Latency (lower is better)</h3>

<svg id="summary-latency-histogram" width="1280" height="400"></svg>

<h3>Mean RPS (higher is better)</h3>

<svg id="summary-rps-histogram" width="1280" height="400"></svg>

<h3> Mean REQ 2XX Count (higher is better)</h3>

<svg id="summary-req-histogram" width="1280" height="400"></svg>

<h1> Framework Results Summary Over Different Modes</h1>

{% for mode in modes_data %}
<h2><span class="uppercase">{{ mode.name }}</span> mode</h2>

<div id="{{ mode.name }}-summary-table"></div>

{% endfor %}

<h2> Mean Latency Over Test Modes</h2>
<svg id="mc-latency-graph" width="1280" height="400"></svg>

<h2> Mean CPU % Over Test Modes</h2>
<svg id="mc-cpu-graph" width="1280" height="400"></svg>

<h2> Mean Memory MB Over Test Modes</h2>
<svg id="mc-memory-graph" width="1280" height="400"></svg>

<h2> Mean RPS Over Test Modes</h2>
<svg id="mc-rps-graph" width="1280" height="400"></svg>

<!-- Render tables / figures here (use functions from main.js) -->
<script>
    const benchmark_results_summary_table_data = {{ benchmark_results_summary_table_data | tojson }};
    render_table(benchmark_results_summary_table_data, "#benchmark-summary-table", 2)

    const summary_latency_histogram_data = {{ summary_latency_histogram_data | tojson }};
    render_histogram("#summary-latency-histogram", summary_latency_histogram_data, "Framework", "Latency Mean (us)", "asc");

    const summary_rps_histogram_data = {{ summary_rps_histogram_data | tojson }};
    render_histogram("#summary-rps-histogram", summary_rps_histogram_data, "Framework", "RPS Mean", "desc");

    const summary_req_histogram_data = {{ summary_req_histogram_data | tojson }};
    render_histogram("#summary-req-histogram", summary_req_histogram_data, "Framework", "REQ 2XX Mean Amount", "desc");

    {% for mode in modes_data %}
    const {{mode.name}}_summary_table_data = {{ mode.data | tojson }};
    render_table({{mode.name}}_summary_table_data, "#{{mode.name}}-summary-table", 2)
    {% endfor %}

    {
        const x_labels = {{ ms_latency_graph.x_labels | tojson }};
        const curves = {{ ms_latency_graph.curves | tojson }};
        const curve_labels = {{ ms_latency_graph.curve_labels | tojson }};
        const main_x_label = "{{ ms_latency_graph.main_x_label }}"
        const main_y_label = "{{ ms_latency_graph.main_y_label }}"
        render_curves_graph("#mc-latency-graph", curves, x_labels, curve_labels, main_x_label, main_y_label);
    }

    {
        const x_labels = {{ ms_cpu_graph.x_labels | tojson }};
        const curves = {{ ms_cpu_graph.curves | tojson }};
        const curve_labels = {{ ms_cpu_graph.curve_labels | tojson }};
        const main_x_label = "{{ ms_cpu_graph.main_x_label }}"
        const main_y_label = "{{ ms_cpu_graph.main_y_label }}"
        render_curves_graph("#mc-cpu-graph", curves, x_labels, curve_labels, main_x_label, main_y_label);
    }

    {
        const x_labels = {{ ms_memory_graph.x_labels | tojson }};
        const curves = {{ ms_memory_graph.curves | tojson }};
        const curve_labels = {{ ms_memory_graph.curve_labels | tojson }};
        const main_x_label = "{{ ms_memory_graph.main_x_label }}"
        const main_y_label = "{{ ms_memory_graph.main_y_label }}"
        render_curves_graph("#mc-memory-graph", curves, x_labels, curve_labels, main_x_label, main_y_label);
    }

    {
        const x_labels = {{ ms_rps_graph.x_labels | tojson }};
        const curves = {{ ms_rps_graph.curves | tojson }};
        const curve_labels = {{ ms_rps_graph.curve_labels | tojson }};
        const main_x_label = "{{ ms_rps_graph.main_x_label }}"
        const main_y_label = "{{ ms_rps_graph.main_y_label }}"
        render_curves_graph("#mc-rps-graph", curves, x_labels, curve_labels, main_x_label, main_y_label);
    }


</script>

{% endblock %}
